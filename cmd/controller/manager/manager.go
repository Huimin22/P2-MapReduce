package manager

import (
	"bufio"
	"bytes"
	pclient "dfs/protogen/client" // Import client types with alias
	"dfs/protogen/common"
	"dfs/protogen/node"
	pnode "dfs/protogen/node" // Import node types with alias
	"dfs/rpc/client"
	"dfs/utils"
	"errors"
	"fmt"
	"hash/fnv"
	"log"
	"math/rand/v2"
	"net"
	"os"
	"os/user"
	"path/filepath"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic" // For task IDs
	"time"
)

// ================= MapReduce Types ==================

type TaskStatus int

const (
	TaskStatusPending TaskStatus = iota
	TaskStatusRunning
	TaskStatusCompleted
	TaskStatusFailed
)

type MapTaskState struct {
	TaskID         uint32
	Status         TaskStatus
	InputChunkID   uint64
	AssignedNodeID string // Which node is running/ran this task
	StartTime      time.Time
	EndTime        time.Time
	OutputKeys     []string // Intermediate file keys generated by this task
	Error          string
}

type ReduceTaskState struct {
	TaskID           uint32
	Status           TaskStatus
	IntermediateKeys []string // Input keys for this reduce task
	AssignedNodeID   string
	StartTime        time.Time
	EndTime          time.Time
	OutputKey        string // Final output key for this task
	Error            string
}

type JobStatus int

const (
	JobStatusRunning JobStatus = iota
	JobStatusCompleted
	JobStatusFailed
)

type MapReduceJobState struct {
	JobID               string
	Status              JobStatus
	InputKey            string
	OutputKey           string
	MapFuncID           string
	ReduceFuncID        string
	ControllerAggregate bool //

	NumMapTasks    uint32
	NumReduceTasks uint32 // Let's fix this for now, maybe configurable later

	MapTasks    map[uint32]*MapTaskState    // map[task_id]*MapTaskState
	ReduceTasks map[uint32]*ReduceTaskState // map[task_id]*ReduceTaskState

	PendingMapTasks    chan uint32 // Channel of map task IDs ready to be scheduled
	PendingReduceTasks chan uint32 // Channel of reduce task IDs ready to be scheduled

	mu        sync.Mutex // Protects access to this job's state
	StartTime time.Time
	EndTime   time.Time
	Error     string

	mapOutputsPerReducer map[uint32][]string // map[reducer_id][]intermediate_keys
	completedMapTasks    atomic.Uint32
	completedReduceTasks atomic.Uint32
	TempDir              string // Directory to store intermediate files
}

// StorageNode is the node that stores the chunks
type StorageNode struct {
	ID            string
	Address       string
	LastHeartbeat time.Time
	// chunks is the chunks that this node has
	// if the value is true, the chunk is available
	// otherwise, the chunk is about to be uploaded
	chunks        map[uint64]bool
	FreeSpace     uint64 // free space, gathered from node
	TotalSpace    uint64 // total space, gathered from node
	RequiredSpace uint64 // required space for upload new files

	// management
	wg  sync.WaitGroup
	m   *Manager
	cli *client.Client
}

// Manager is the manager of the storage nodes, and chunks
type Manager struct {
	mu sync.Mutex

	nodes         map[string]*StorageNode
	chunks        map[uint64]*ChunkInfo
	files         map[string]FileMetadata
	candiateFiles map[string]FileMetadata

	// constant
	chunkSize    uint64
	replicaCount uint64

	wg    sync.WaitGroup
	close chan struct{}

	mapReduceJobs  map[string]*MapReduceJobState
	pluginDir      string            // Directory to store plugins locally
	pluginRegistry map[string]string // map[plugin_id]filepath
}

// closeTimer is a helper function to close a timer
func closeTimer(t *time.Timer) {
	if !t.Stop() {
		<-t.C
	}
}

// NewManager creates a new manager
func NewManager(chunkSize uint64, replicaCount uint64) *Manager {
	// Determine plugin directory (e.g., relative to executable or fixed path)
	// For simplicity, let's put it in the controller's working dir
	pluginStoragePath := "./controller_plugins"
	if err := os.MkdirAll(pluginStoragePath, 0755); err != nil {
		// Handle error better in production
		log.Printf("Warning: Could not create plugin directory %s: %v", pluginStoragePath, err)
	}

	m := &Manager{
		nodes:          make(map[string]*StorageNode),
		chunks:         make(map[uint64]*ChunkInfo),
		files:          make(map[string]FileMetadata),
		candiateFiles:  make(map[string]FileMetadata),
		chunkSize:      chunkSize,
		replicaCount:   replicaCount,
		close:          make(chan struct{}),
		mapReduceJobs:  make(map[string]*MapReduceJobState),
		pluginDir:      pluginStoragePath,
		pluginRegistry: make(map[string]string),
	}
	m.wg.Add(3)
	go m.CheckCandiateFiles()
	go m.CheckNodeHealth()
	go m.MaintainChunkDuplicate()
	return m
}

// Close closes the manager
func (m *Manager) Close() {
	close(m.close)
	m.wg.Wait()
}

// preRemoveDeadNode the hook to remove dead node
// the lock is already acquired
func (m *Manager) preRemoveDeadNode(node *StorageNode) {
	// go through the chunks and remove the node from the chunk
	for chunk := range node.chunks {
		m.chunks[chunk].removeReplica(node.ID)
	}
}

// removeChunk removes the chunk from the node
func (sn *StorageNode) removeChunk(chunkId uint64) bool {
	chunk, ok := sn.m.chunks[chunkId]
	if !ok {
		panic(fmt.Sprintf("chunk %d not found", chunkId))
	}
	if v, ok := sn.chunks[chunkId]; ok {
		if !v {
			sn.RequiredSpace -= chunk.Size
		}
		delete(sn.chunks, chunkId)
		return v
	}
	return false
}

// runAsyncTask runs the passed function asynchronously
func (sn *StorageNode) runAsyncTask(f func(sn *StorageNode) error) {
	sn.wg.Add(1)
	go func() {
		defer sn.wg.Done()
		defer func() {
			if r := recover(); r != nil {
				log.Println("recovered from panic:", r)
			}
		}()
		if err := f(sn); err != nil {
			log.Println("error in async task:", err)
		}
	}()
}

// setChunkStored sets the givne chunk as stored
func (sn *StorageNode) setChunkStored(chunkId uint64) {
	v, ok := sn.chunks[chunkId]
	if ok && !v {
		sn.RequiredSpace -= sn.m.chunks[chunkId].Size
	}
	sn.chunks[chunkId] = true
}

// weight returns the storage node weight,
// which is the ratio of free space to total space
func (sn *StorageNode) weight() float32 {
	free := float32(sn.FreeSpace)
	if free <= 0 || sn.TotalSpace <= 0 {
		return 0
	}
	return free / float32(sn.TotalSpace)
}

// addChunk adds the chunk to the node
func (sn *StorageNode) addChunk(chunkId uint64, dummy bool) {
	if v, ok := sn.chunks[chunkId]; ok {
		if v == dummy {
			panic("should not happen")
		}
		// pass
		return
	}
	sn.chunks[chunkId] = !dummy
	if dummy {
		sn.RequiredSpace += sn.m.chunks[chunkId].Size
	}
}

// selectReplicaNode selects 'count' replicas for the given chunk
func (m *Manager) selectReplicaNode(ci *ChunkInfo, count int) ([]string, error) {
	type nodeWeight struct {
		node   *StorageNode
		weight float32
	}
	var nodeWeights []nodeWeight
	replicas := ci.Replicas.ToSet()
	for _, node := range m.nodes {
		if _, ok := replicas[node.ID]; ok {
			continue
		}
		w := node.weight()
		if node.FreeSpace-node.RequiredSpace < ci.Size || w == 0 {
			continue
		}
		nodeWeights = append(nodeWeights, nodeWeight{
			node:   node,
			weight: w,
		})
	}
	if len(nodeWeights) < count {
		return nil, errors.New("no enough node available")
	}
	sort.Slice(nodeWeights, func(i, j int) bool {
		return nodeWeights[i].weight > nodeWeights[j].weight
	})
	nodes := make([]string, 0, count)
	for i := range count {
		nodes = append(nodes, nodeWeights[i].node.ID)
	}
	return nodes, nil
}

// createChunk creates a new chunk
func (m *Manager) createChunk(chunkSize uint64, fm *FileMetadata) ChunkInfo {
	// generate a chunk id
	for {
		rid := rand.Uint64()
		if _, ok := m.chunks[rid]; !ok {
			return ChunkInfo{
				ChunkID:  rid,
				Replicas: utils.NewUnorderedList[string](),
				Size:     chunkSize,
				fm:       fm,
			}
		}
	}
}

// availableSpace returns the available space
func (m *Manager) availableSpace() uint64 {
	var total uint64
	for _, sn := range m.nodes {
		total += sn.FreeSpace - sn.RequiredSpace
	}
	return total
}

// GetFile returns the file metadata for the given key
func (m *Manager) GetFile(key string) (*FileMetadata, error) {
	m.mu.Lock()
	defer m.mu.Unlock()
	fm, ok := m.files[key]
	if !ok {
		return nil, fmt.Errorf("file %s not found", key)
	}
	fm = fm.Clone()
	return &fm, nil
}

// DeleteFile deletes the file metadata for the given key
func (m *Manager) DeleteFile(key string) (bool, error) {
	m.mu.Lock()
	defer m.mu.Unlock()
	isCandiate := false
	fm, ok := m.files[key]
	if !ok {
		fm, ok = m.candiateFiles[key]
		if !ok {
			return false, fmt.Errorf("file %s not found", key)
		}
		isCandiate = true
	}
	for _, chunk := range fm.Chunks {
		m.preRemoveChunk(chunk.ChunkID)
		delete(m.chunks, chunk.ChunkID)
	}
	if isCandiate {
		delete(m.candiateFiles, key)
	} else {
		delete(m.files, key)
	}
	return isCandiate, nil
}

// ListFiles lists the files with the given prefix
func (m *Manager) ListFiles(prefix string) ([]FileMetadata, error) {
	m.mu.Lock()
	defer m.mu.Unlock()
	ans := make([]FileMetadata, 0)
	for key, fm := range m.files {
		if strings.HasPrefix(key, prefix) {
			ans = append(ans, fm)
		}
	}
	return ans, nil
}

// FinishStoreFile finishes the store file
func (m *Manager) FinishStoreFile(key string) error {
	m.mu.Lock()
	defer m.mu.Unlock()
	fm, ok := m.candiateFiles[key]
	if !ok {
		return fmt.Errorf("file %s not found", key)
	}
	// check if the file is finished
	for _, chunk := range fm.Chunks {
		if !m.nodes[chunk.Replicas.GetUnderlyingList()[0]].chunks[chunk.ChunkID] {
			return fmt.Errorf("chunk %d is not finished", chunk.ChunkID)
		}
	}
	delete(m.candiateFiles, key)
	m.files[key] = fm
	return nil
}

// StoreFile creates the file metadata for a file with given key to be stored
func (m *Manager) StoreFile(key string, size uint64, isText bool) (*FileMetadata, error) {
	chunks := (size + m.chunkSize - 1) / m.chunkSize
	m.mu.Lock()
	defer m.mu.Unlock()
	if _, ok := m.files[key]; ok {
		return nil, fmt.Errorf("file %s already exists", key)
	}
	if _, ok := m.candiateFiles[key]; ok {
		return nil, fmt.Errorf("file %s is about to be uploaded, you can delete the tmp file by 'delete' command", key)
	}
	if space := m.availableSpace(); chunks > space {
		return nil, fmt.Errorf("manager: not enough chunks available, need %d, but only %d available", chunks, space)
	}
	// we can store the file
	fm := FileMetadata{
		Name:      key,
		Size:      size,
		CreatedAt: time.Now(),
		IsText:    isText,
	}
	for i := uint64(0); i < chunks; i++ {
		var chunkSize uint64
		if i == chunks-1 {
			chunkSize = size - (m.chunkSize * i)
		} else {
			chunkSize = m.chunkSize
		}
		ci := m.createChunk(chunkSize, &fm)
		nodes, err := m.selectReplicaNode(&ci, int(m.replicaCount))
		if err != nil {
			m.removeDeadFile(fm)
			return nil, err
		}
		m.chunks[ci.ChunkID] = &ci
		for _, nodeId := range nodes {
			ci.addReplica(nodeId)
			m.nodes[nodeId].addChunk(ci.ChunkID, true)
		}
		fm.Chunks = append(fm.Chunks, ci)
	}
	m.candiateFiles[key] = fm
	// clone the file metadata
	fm = fm.Clone()
	return &fm, nil
}

// generateNodeId generates the node id
// it must be the node address for now
func (m *Manager) generateNodeId(address string) string {
	return address
}

// AddNode adds a new node to the manager
func (m *Manager) AddNode(address string) (s StorageNode, err error) {
	nodeId := m.generateNodeId(address)
	m.mu.Lock()
	defer m.mu.Unlock()
	if _, ok := m.nodes[nodeId]; ok {
		err = fmt.Errorf("node %s already exists", nodeId)
		return
	}
	var conn net.Conn
	conn, err = net.Dial("tcp", address)
	if err != nil {
		err = fmt.Errorf("failed to connect to node %s: %w", address, err)
		return
	}
	s = StorageNode{
		ID:            nodeId,
		Address:       address,
		LastHeartbeat: time.Now(),
		chunks:        make(map[uint64]bool),
		cli:           client.NewClient(conn),
	}
	s.m = m
	m.nodes[nodeId] = &s
	return
}

// UpdateHeartbeat updates the heartbeat of the node and returns
// the chunks that should be removed.
func (m *Manager) UpdateHeartbeat(nodeId string, metadata *node.Metadata) ([]uint64, error) {
	m.mu.Lock()
	defer m.mu.Unlock()
	node, ok := m.nodes[nodeId]
	if !ok {
		return nil, fmt.Errorf("node %s not found", nodeId)
	}

	chunks := make(map[uint64]struct{}, len(metadata.Chunks))

	// do something for the chunk fields
	removeChunks := make([]uint64, 0)
	for _, chunk := range metadata.Chunks {
		chunks[chunk] = struct{}{}
		if v, ok := node.chunks[chunk]; !ok {
			// the chunk should be removed
			removeChunks = append(removeChunks, chunk)
		} else {
			if v {
				// the chunks is stored
				// pass
			} else {
				node.setChunkStored(chunk)
			}
		}
	}
	// remove unstored chunks
	for chunkId := range node.chunks {
		if _, ok := chunks[chunkId]; !ok {
			m.chunks[chunkId].removeReplica(nodeId)
			node.removeChunk(chunkId)
		}
	}

	node.LastHeartbeat = time.Now()
	node.TotalSpace = metadata.Totalspace
	node.FreeSpace = metadata.Freespace

	return removeChunks, nil
}

// UpdateChunkUploaded updates the chunk metadata and returns the error.
func (m *Manager) UpdateChunkUploaded(chunkId uint64, hash string, size uint64, replicas []string) error {
	m.mu.Lock()
	defer m.mu.Unlock()
	c, ok := m.chunks[chunkId]
	if !ok {
		return fmt.Errorf("chunk %d not found", chunkId)
	}
	if c.Size != size {
		return fmt.Errorf("chunk %d size mismatch", chunkId)
	}
	has := c.Replicas.ToSet()
	for _, node := range replicas {
		if nd, ok := m.nodes[node]; ok {
			nd.setChunkStored(chunkId)
			if _, ok := has[node]; !ok {
				c.Replicas.AddNoDuplicate(node)
			}
		}
	}
	c.Hash = hash
	return nil
}

// preRemoveChunk removes the chunk from the node and send the request
// to the node to remove the chunk if necessary.
func (m *Manager) preRemoveChunk(chunkId uint64) {
	if c, ok := m.chunks[chunkId]; ok {
		for _, nodeId := range c.Replicas.GetUnderlyingList() {
			nd := m.nodes[nodeId]
			if exist := nd.removeChunk(chunkId); !exist {
				continue
			}
			nd.runAsyncTask(func(sn *StorageNode) error {
				_, err := sn.cli.SendRequest(&node.DeleteChunk{Id: chunkId})
				return err
			})
		}
	}
}

// removeDeadFile removes the file from the manager and the node.
func (m *Manager) removeDeadFile(fm FileMetadata) {
	for _, chunk := range fm.Chunks {
		m.preRemoveChunk(chunk.ChunkID)
		delete(m.chunks, chunk.ChunkID)
	}
	delete(m.candiateFiles, fm.Name)
}

// GetNode returns the node by nodeId.
func (m *Manager) GetNode(nodeId string) (*StorageNode, error) {
	m.mu.Lock()
	defer m.mu.Unlock()
	if node, exist := m.nodes[nodeId]; exist {
		return node, nil
	} else {
		return nil, errors.New("node not found")
	}
}

// ListNodes returns the list of nodes.
func (m *Manager) ListNodes() ([]*StorageNode, error) {
	m.mu.Lock()
	defer m.mu.Unlock()
	nodes := make([]*StorageNode, 0, len(m.nodes))
	for _, node := range m.nodes {
		nodes = append(nodes, node)
	}
	return nodes, nil
}

// CheckCandiateFiles checks the candiate files and remove the dead files,
// which means the file is not uploaded within 30 minutes.
func (m *Manager) CheckCandiateFiles() {
	defer m.wg.Done()
	const duration = time.Minute
	for t := time.NewTimer(duration); true; t.Reset(duration) {
		select {
		case <-m.close:
			closeTimer(t)
			return
		case <-t.C:
		}
		var deadFiles []string
		exp := time.Now().Add(-30 * time.Minute)
		m.mu.Lock()
		for key, file := range m.candiateFiles {
			if file.CreatedAt.Before(exp) {
				deadFiles = append(deadFiles, key)
			}
		}
		for _, key := range deadFiles {
			m.removeDeadFile(m.candiateFiles[key])
		}
		m.mu.Unlock()
	}
}

// CheckNodeHealth checks the node health and remove the dead nodes.
func (m *Manager) CheckNodeHealth() {
	defer m.wg.Done()
	const duration = 10 * time.Second
	for t := time.NewTimer(duration); true; t.Reset(duration) {
		select {
		case <-m.close:
			closeTimer(t)
			return
		case <-t.C:
		}
		var deadNodes []string
		now := time.Now()
		m.mu.Lock()
		for _, node := range m.nodes {
			if now.Sub(node.LastHeartbeat) > 30*time.Second {
				deadNodes = append(deadNodes, node.ID)
			}
		}
		if len(deadNodes) > 0 {
			fmt.Printf("[controller] %d nodes are dead: %v\n", len(deadNodes), deadNodes)
		}
		for _, nodeID := range deadNodes {
			m.preRemoveDeadNode(m.nodes[nodeID])
			sn := m.nodes[nodeID]
			go func(sn *StorageNode) {
				sn.wg.Wait()
				sn.cli.Close()
			}(sn)
			delete(m.nodes, nodeID)
		}
		m.mu.Unlock()
	}
}

// MaintainChunkDuplicate maintains the chunk replicas.
func (m *Manager) MaintainChunkDuplicate() {
	defer m.wg.Done()
	const duration = 20 * time.Second
	for t := time.NewTimer(duration); true; t.Reset(duration) {
		select {
		case <-m.close:
			closeTimer(t)
			return
		case <-t.C:
		}
		var chunksNeedDuplicate [][]uint64
		m.mu.Lock()
		counts := 0
		// only check the file that is already stored on server
		for _, fm := range m.files {
			for _, chunk := range fm.Chunks {
				if chunk.Replicas.Len() < int(m.replicaCount) {
					needs := int(m.replicaCount) - chunk.Replicas.Len()
					for needs >= len(chunksNeedDuplicate) {
						chunksNeedDuplicate = append(chunksNeedDuplicate, []uint64{})
					}
					chunksNeedDuplicate[needs] = append(chunksNeedDuplicate[needs], chunk.ChunkID)
					counts++
				}
			}
		}
		if len(chunksNeedDuplicate) == 0 {
			m.mu.Unlock()
			continue
		}
		fmt.Printf("[controller] detected %d chunks need duplicate\n", counts)
		type info struct {
			chunkId  uint64
			replicas []string
		}
		// select the node that can store the chunk and gather the
		// tasks for each node, which can prevent from sending too many
		// RPC requests to the same node
		sends := make(map[string][]info)

	outer:
		for i, chunks := range chunksNeedDuplicate {
			if i == 0 {
				continue
			}
			for _, chunk := range chunks {
				c := m.chunks[chunk]
				replicas, err := m.selectReplicaNode(c, i)
				if err != nil {
					fmt.Printf("[controller] faild to select replica node for %d: %v\n", chunk, err)
					break outer
				}
				servers := c.Replicas.GetUnderlyingList()
				if len(servers) == 0 {
					panic(fmt.Sprintf("[controller] faild to find a server that contains chunkd %d\n", chunk))
				}
				server := servers[0]
				sends[server] = append(sends[server], info{
					chunkId:  chunk,
					replicas: replicas,
				})
			}
		}
		for server, chunks := range sends {
			fmt.Printf("[controller] create send %d chunk task(s) to node %s\n", len(chunks), server)
			// this will be okay as the chunk is a copy after go 1.21
			// use async task to avoid deadlock and other issues
			m.nodes[server].runAsyncTask(func(sn *StorageNode) error {
				cs := make([]*node.SendChunk, 0, len(chunks))
				for _, c := range chunks {
					cs = append(cs, &node.SendChunk{
						Id:       c.chunkId,
						Replicas: c.replicas,
					})
				}
				_, err := sn.cli.SendRequest(&node.SendChunks{
					Chunks: cs,
				})
				return err
			})
		}
		m.mu.Unlock()
	}
}
func getUsername() string {
	u, err := user.Current()
	if err != nil {
		panic(err)
	}
	return u.Username
}

// StartMapReduceJob validates the request and prepares the MapReduce job.
// It creates the initial job state and map/reduce task structures.
func (m *Manager) StartMapReduceJob(inputKey, outputKey, mapFuncId, reduceFuncId string, controllerAggregate bool, numReduceTasks uint32) (string, error) {
	m.mu.Lock()
	// We release the main manager lock quickly after initial validation and job creation,
	// relying on the job-specific lock for further state changes.

	// 1. Validate input file
	inputFileMeta, ok := m.files[inputKey]
	if !ok {
		m.mu.Unlock()
		return "", fmt.Errorf("input file '%s' not found", inputKey)
	}

	// 2. Check if input file is text
	if !inputFileMeta.IsText {
		m.mu.Unlock()
		return "", fmt.Errorf("input file '%s' is not a text file, cannot run MapReduce", inputKey)
	}

	// 3. Validate output file key
	if _, ok := m.files[outputKey]; ok {
		m.mu.Unlock()
		return "", fmt.Errorf("output file '%s' already exists", outputKey)
	}
	if _, ok := m.candiateFiles[outputKey]; ok {
		m.mu.Unlock()
		return "", fmt.Errorf("output file '%s' is currently being created by another upload", outputKey)
	}

	// 4. Validate map/reduce function IDs (placeholder)
	if mapFuncId == "" || reduceFuncId == "" {
		m.mu.Unlock()
		return "", errors.New("map and reduce function IDs must be provided")
	}

	// 5. Validate numReduceTasks
	if numReduceTasks == 0 {
		// default as 4 tasks
		numReduceTasks = 4
		log.Printf("[Controller] No reducer count specified, using default value: %d", numReduceTasks)
	}

	// 6. Create a unique job ID
	jobID := fmt.Sprintf("job-%d-%d", time.Now().UnixNano(), rand.Int64())

	// Modify the inputKey and outputKey paths if they don't have absolute paths
	// Store intermediate files in temp directory
	tempDir := "/bigdata/students/" + getUsername() + "/temp"

	// Create temp directory if it doesn't exist
	if _, err := os.Stat(tempDir); os.IsNotExist(err) {
		err := os.Mkdir(tempDir, 0755)
		if err != nil {
			return "", fmt.Errorf("failed to create temp directory: %w", err)
		}
	}

	// 7. Create and Register Job State
	jobState := &MapReduceJobState{
		JobID:                jobID,
		Status:               JobStatusRunning,
		InputKey:             inputKey,
		OutputKey:            outputKey,
		MapFuncID:            mapFuncId,
		ReduceFuncID:         reduceFuncId,
		ControllerAggregate:  controllerAggregate,
		NumMapTasks:          uint32(len(inputFileMeta.Chunks)),
		NumReduceTasks:       numReduceTasks,
		MapTasks:             make(map[uint32]*MapTaskState),
		ReduceTasks:          make(map[uint32]*ReduceTaskState),
		PendingMapTasks:      make(chan uint32, len(inputFileMeta.Chunks)),
		PendingReduceTasks:   make(chan uint32, numReduceTasks),
		StartTime:            time.Now(),
		mapOutputsPerReducer: make(map[uint32][]string),
		TempDir:              tempDir,
	}

	// Create Map Task States
	for i, chunk := range inputFileMeta.Chunks {
		taskID := uint32(i)
		mapTask := &MapTaskState{
			TaskID:       taskID,
			Status:       TaskStatusPending,
			InputChunkID: chunk.ChunkID,
		}
		jobState.MapTasks[taskID] = mapTask
		jobState.PendingMapTasks <- taskID // Add task ID to pending queue
	}
	close(jobState.PendingMapTasks) // Close channel after adding all initial tasks

	// Create Reduce Task States
	for i := uint32(0); i < numReduceTasks; i++ {
		taskID := i

		// Set the output file path for the reducer in the temp directory
		var outputKey string
		if controllerAggregate {
			// If the controller will aggregate results, place the reduce output in the temp directory
			outputKey = fmt.Sprintf("%s/%s-reduce-%d", tempDir, jobID, taskID)
		} else {
			//  Otherwise, use the final output format, but still place it in the temp directory
			outputKey = fmt.Sprintf("%s/%s-output-reduce-%d", tempDir, jobID, taskID)
		}

		reduceTask := &ReduceTaskState{
			TaskID: taskID,
			Status: TaskStatusPending, // Will wait for map tasks
			// IntermediateKeys will be populated as map tasks complete
			OutputKey: outputKey, // Use a path within the temp directory
		}
		jobState.ReduceTasks[taskID] = reduceTask
		jobState.mapOutputsPerReducer[taskID] = make([]string, 0)
		// Don't add to pending queue yet - reducers wait for mappers
	}

	// Store the job state
	m.mapReduceJobs[jobID] = jobState
	m.mu.Unlock() // Release manager lock

	log.Printf("[Controller] Initialized MapReduce Job '%s': Input='%s', Output='%s', MapTasks=%d, ReduceTasks=%d",
		jobID, inputKey, outputKey, len(inputFileMeta.Chunks), numReduceTasks)

	// TODO: Start a goroutine to manage this specific job (scheduling tasks, monitoring, etc.)
	go m.manageJob(jobState)

	return jobID, nil
}

// manageJob orchestrates the execution of a MapReduce job.
func (m *Manager) manageJob(job *MapReduceJobState) {
	log.Printf("[Job %s] Starting management goroutine", job.JobID)
	job.mu.Lock()
	if job.Status != JobStatusRunning { // Check if job already failed during init
		log.Printf("[Job %s] Job status is %v, not starting management loops.", job.JobID, job.Status)
		job.mu.Unlock()
		return
	}
	job.mu.Unlock()

	var wg sync.WaitGroup
	jobDone := make(chan struct{})

	// Goroutine to schedule map tasks
	wg.Add(1)
	go func() {
		defer wg.Done()
		log.Printf("[Job %s] Starting map task scheduling loop.", job.JobID)
		for mapTaskID := range job.PendingMapTasks {
			// Check if job has failed before scheduling
			job.mu.Lock()
			status := job.Status
			job.mu.Unlock()
			if status != JobStatusRunning {
				log.Printf("[Job %s] Job failed/completed, stopping map task scheduling.", job.JobID)
				break
			}

			log.Printf("[Job %s] Attempting to schedule map task %d", job.JobID, mapTaskID)
			scheduled := m.scheduleMapTask(job, mapTaskID)
			if !scheduled {
				log.Printf("[Job %s] Failed to schedule map task %d immediately, will retry later?", job.JobID, mapTaskID)
				// Simple retry logic: put back into channel after a delay
				// Warning: This can lead to head-of-line blocking if nodes are persistently unavailable.
				// A better approach might use a separate retry queue or exponential backoff.
				time.Sleep(5 * time.Second) // Delay before re-queueing
				go func(id uint32) {
					// Need to re-check job status before re-queueing
					job.mu.Lock()
					status := job.Status
					job.mu.Unlock()
					if status == JobStatusRunning {
						// Ensure task is still pending before requeuing
						job.mu.Lock()
						if job.MapTasks[id].Status == TaskStatusPending {
							log.Printf("[Job %s] Re-queueing map task %d for scheduling attempt.", job.JobID, id)
							job.PendingMapTasks <- id
						}
						job.mu.Unlock()
					}
				}(mapTaskID)
			}
		}
		log.Printf("[Job %s] Finished map task scheduling loop.", job.JobID)
	}()

	// Goroutine to schedule reduce tasks
	wg.Add(1)
	go func() {
		defer wg.Done()
		log.Printf("[Job %s] Starting reduce task scheduling loop (waiting for tasks).", job.JobID)
		for reduceTaskID := range job.PendingReduceTasks { // Blocks until map tasks finish and HandleTaskCompletion sends IDs
			// Check if job has failed before scheduling
			job.mu.Lock()
			status := job.Status
			job.mu.Unlock()
			if status != JobStatusRunning {
				log.Printf("[Job %s] Job failed/completed, stopping reduce task scheduling.", job.JobID)
				break
			}

			log.Printf("[Job %s] Attempting to schedule reduce task %d", job.JobID, reduceTaskID)
			scheduled := m.scheduleReduceTask(job, reduceTaskID)
			if !scheduled {
				log.Printf("[Job %s] Failed to schedule reduce task %d immediately, will retry later?", job.JobID, reduceTaskID)
				// Simple retry logic (similar to map tasks)
				time.Sleep(5 * time.Second)
				go func(id uint32) {
					job.mu.Lock()
					status := job.Status
					job.mu.Unlock()
					if status == JobStatusRunning {
						job.mu.Lock()
						if job.ReduceTasks[id].Status == TaskStatusPending {
							log.Printf("[Job %s] Re-queueing reduce task %d for scheduling attempt.", job.JobID, id)
							job.PendingReduceTasks <- id
						}
						job.mu.Unlock()
					}
				}(reduceTaskID)
			}
		}
		log.Printf("[Job %s] Finished reduce task scheduling loop.", job.JobID)
	}()

	// Goroutine to wait for scheduler goroutines to finish and signal completion
	go func() {
		wg.Wait()      // Wait for map and reduce scheduling loops to exit
		close(jobDone) // Signal that scheduling attempts are done
	}()

	// Main loop: Monitor job status until completion or failure
monitorLoop:
	for {
		job.mu.Lock()
		currentStatus := job.Status
		job.mu.Unlock()

		if currentStatus == JobStatusCompleted || currentStatus == JobStatusFailed {
			log.Printf("[Job %s] Detected final status: %v. Exiting monitor loop.", job.JobID, currentStatus)
			break monitorLoop
		}
		select {
		case <-jobDone:
			// Scheduling goroutines finished. Check final status one last time.
			log.Printf("[Job %s] Task scheduling goroutines finished.", job.JobID)
			jobDone = nil // Set channel to nil to prevent re-selection in future loops
		case <-time.After(10 * time.Second): // Check status periodically
			job.mu.Lock()
			status := job.Status
			job.mu.Unlock()
			log.Printf("[Job %s] Monitoring... Status: %v", job.JobID, status)
			if status == JobStatusCompleted || status == JobStatusFailed {
				log.Printf("[Job %s] Detected final status: %v. Exiting monitor loop.", job.JobID, status)
				break monitorLoop
			}
		}
	}

	// Final job status check and cleanup
	job.mu.Lock()
	finalStatus := job.Status
	if finalStatus == JobStatusRunning {
		// Should not happen if loops exited correctly, but handle defensively
		log.Printf("[Job %s] WARNING: manageJob exiting but job status is still Running. Marking as Failed.", job.JobID)
		job.Status = JobStatusFailed
		job.Error = "Job management finished unexpectedly while still running"
		job.EndTime = time.Now()
	}
	log.Printf("[Job %s] Management goroutine finished. Final Status: %v, Error: %s", job.JobID, job.Status, job.Error)
	job.mu.Unlock()

	// If the job fails, clean up intermediate files
	if finalStatus == JobStatusFailed {
		log.Printf("[Job %s] Job failed, cleaning up intermediate files", job.JobID)
		m.cleanupJobFiles(job)
		return
	}

	// If the job completes and aggregation is required, perform aggregation and clean up
	if finalStatus == JobStatusCompleted && job.ControllerAggregate {
		log.Printf("[Job %s] Starting final output aggregation.", job.JobID)
		//Aggregate the results
		err := m.aggregateFinalOutput(job)
		if err != nil {
			log.Printf("[Job %s] CRITICAL: Job completed but failed to aggregate final output: %v. Manual cleanup may be required.", job.JobID, err)
			// Mark job as failed after the fact? Or leave completed but log error?
			// Let's log prominently but keep status Completed for now.
			job.mu.Lock()
			job.Error = fmt.Sprintf("Aggregation failed after completion: %v", err)
			job.mu.Unlock()
		} else {
			log.Printf("[Job %s] Successfully aggregated final output to key '%s'.", job.JobID, job.OutputKey)
		}

		// Clean up intermediate files after aggregation is complete
		log.Printf("[Job %s] Aggregation finished, cleaning up intermediate files", job.JobID)
		m.cleanupJobFiles(job)
	}

	// TODO: Optional: Remove job state from m.mapReduceJobs after some time or based on status?
}

// aggregateFinalOutput downloads reducer outputs, merges them (optionally aggregates), and uploads the final result.
func (m *Manager) aggregateFinalOutput(job *MapReduceJobState) error {
	log.Printf("[Job %s] Aggregating output from %d reducers for final key '%s' (ControllerAggregate: %t)",
		job.JobID, job.NumReduceTasks, job.OutputKey, job.ControllerAggregate)

	var finalDataBuffer bytes.Buffer
	reducerOutputKeys := make([]string, 0, job.NumReduceTasks)

	job.mu.Lock() // Lock needed to access ReduceTasks map safely
	for i := uint32(0); i < job.NumReduceTasks; i++ {
		reduceTask, ok := job.ReduceTasks[i]
		if !ok {
			log.Printf("[Job %s] Warning: Reduce task %d state not found during aggregation.", job.JobID, i)
			continue
		}
		if reduceTask.Status != TaskStatusCompleted {
			job.mu.Unlock() // Unlock before returning error
			return fmt.Errorf("cannot aggregate: reduce task %d did not complete successfully (status: %v)", i, reduceTask.Status)
		}
		if reduceTask.OutputKey == "" {
			log.Printf("[Job %s] Info: Reduce task %d completed but has no output key recorded (likely no input). Skipping.", job.JobID, i)
			continue
		}
		reducerOutputKeys = append(reducerOutputKeys, reduceTask.OutputKey)
	}
	job.mu.Unlock()

	if len(reducerOutputKeys) == 0 {
		log.Printf("[Job %s] No reducer output keys found to aggregate.", job.JobID)
		return m.internalUploadData("localhost:8080", job.OutputKey, []byte{})
	}

	// --- Download Phase ---
	controllerClientAddr := "localhost:8080" // Assume default
	log.Printf("[Job %s] Downloading %d reducer output files...", job.JobID, len(reducerOutputKeys))
	downloadedData := make(map[string][]byte, len(reducerOutputKeys))
	for _, key := range reducerOutputKeys {
		log.Printf("[Job %s] Downloading reducer output '%s'...", job.JobID, key)
		data, err := m.internalDownloadData(controllerClientAddr, key)
		if err != nil {
			return fmt.Errorf("failed to download reducer output key '%s': %w", key, err)
		}
		downloadedData[key] = data
	}

	// --- Aggregation/Concatenation Phase ---
	var finalData []byte
	if !job.ControllerAggregate { // <<< Check the flag
		// Simple Concatenation (Default Behavior)
		log.Printf("[Job %s] Concatenating reducer outputs.", job.JobID)
		sort.Strings(reducerOutputKeys) // Sort for deterministic concatenation order
		for _, key := range reducerOutputKeys {
			finalDataBuffer.Write(downloadedData[key])
		}
		finalData = finalDataBuffer.Bytes()
	} else {
		// Controller-side Aggregation (New Behavior)
		log.Printf("[Job %s] Performing controller-side aggregation.", job.JobID)
		aggregatedCounts := make(map[string]int)
		lineCount := 0
		parseErrors := 0

		for key, data := range downloadedData {
			scanner := bufio.NewScanner(bytes.NewReader(data))
			for scanner.Scan() {
				line := scanner.Text()
				lineCount++
				parts := strings.SplitN(line, "\t", 2)
				if len(parts) != 2 {
					log.Printf("[Job %s] Warning: Malformed line in '%s': %s", job.JobID, key, line)
					parseErrors++
					continue
				}
				word := parts[0]
				countStr := parts[1]
				count, err := strconv.Atoi(countStr)
				if err != nil {
					log.Printf("[Job %s] Warning: Cannot parse count '%s' for word '%s' in '%s': %v", job.JobID, countStr, word, key, err)
					parseErrors++
					continue
				}
				aggregatedCounts[word] += count
			}
			if err := scanner.Err(); err != nil {
				return fmt.Errorf("error scanning data from reducer output key '%s': %w", key, err)
			}
		}
		log.Printf("[Job %s] Aggregation processed %d lines, found %d unique keys, encountered %d parse errors.",
			job.JobID, lineCount, len(aggregatedCounts), parseErrors)

		// Format the final aggregated output (sorted)
		keys := make([]string, 0, len(aggregatedCounts))
		for k := range aggregatedCounts {
			keys = append(keys, k)
		}
		sort.Strings(keys)
		for _, k := range keys {
			fmt.Fprintf(&finalDataBuffer, "%s\t%d\n", k, aggregatedCounts[k])
		}
		finalData = finalDataBuffer.Bytes()
	}

	// --- Upload Phase ---
	log.Printf("[Job %s] Uploading final aggregated output to '%s' (Size: %d bytes)...", job.JobID, job.OutputKey, len(finalData))
	err := m.internalUploadData(controllerClientAddr, job.OutputKey, finalData)
	if err != nil {
		return fmt.Errorf("failed to upload final aggregated output to key '%s': %w", job.OutputKey, err)
	}

	// --- Cleanup Phase ---
	log.Printf("[Job %s] Deleting %d intermediate reducer output files...", job.JobID, len(reducerOutputKeys))
	for _, key := range reducerOutputKeys {
		log.Printf("[Job %s] Deleting intermediate file '%s'...", job.JobID, key)
		_, errDel := m.DeleteFile(key)
		if errDel != nil {
			log.Printf("[Job %s] Warning: Failed to delete intermediate reducer output '%s': %v", job.JobID, key, errDel)
		}
	}

	// 清理所有中间文件
	log.Printf("[Job %s] Cleaning up all intermediate files...", job.JobID)
	tempFiles, err := m.ListFiles(fmt.Sprintf("%s/%s", job.TempDir, job.JobID))
	if err != nil {
		log.Printf("[Job %s] Warning: Failed to list intermediate files: %v", job.JobID, err)
	} else {
		for _, file := range tempFiles {
			log.Printf("[Job %s] Deleting intermediate file '%s'...", job.JobID, file.Name)
			_, errDel := m.DeleteFile(file.Name)
			if errDel != nil {
				log.Printf("[Job %s] Warning: Failed to delete intermediate file '%s': %v", job.JobID, file.Name, errDel)
			}
		}
	}

	log.Printf("[Job %s] Aggregation complete.", job.JobID)
	return nil
}

// internalUploadData handles uploading data bytes to the DFS from within the manager.
func (m *Manager) internalUploadData(controllerClientAddr string, key string, data []byte) error {
	log.Printf("[Manager DFS Client] Uploading Key='%s', Size=%d to %s", key, len(data), controllerClientAddr)

	// 1. Connect Controller Client Port
	connCtrl, err := net.Dial("tcp", controllerClientAddr)
	if err != nil {
		return fmt.Errorf("[Manager DFS Client] failed to dial controller client port %s: %w", controllerClientAddr, err)
	}
	ctrlCli := client.NewClient(connCtrl)
	defer ctrlCli.Close()

	// 2. Initial StoreRequest
	fileSize := uint64(len(data))
	storeRespRaw, err := ctrlCli.SendRequest(&pclient.StoreRequest{
		Key:    key,
		Size:   fileSize,
		IsText: false, // Assuming aggregated data isn't necessarily text
	})
	if err != nil {
		return fmt.Errorf("[Manager DFS Client] initial StoreRequest for key '%s' failed: %w", key, err)
	}
	storeResp, ok := storeRespRaw.(*pclient.StoreResponse)
	if !ok {
		return fmt.Errorf("[Manager DFS Client] unexpected response type for initial StoreRequest: %T", storeRespRaw)
	}
	if len(storeResp.Chunks) == 0 && fileSize > 0 {
		return fmt.Errorf("[Manager DFS Client] controller returned no chunks for non-empty file key '%s'", key)
	}
	log.Printf("[Manager DFS Client] Received %d chunk assignments for key '%s'", len(storeResp.Chunks), key)

	// 3. Connect to Storage Nodes & PutChunk (concurrently)
	var wg sync.WaitGroup
	errChan := make(chan error, len(storeResp.Chunks))
	currentPos := uint64(0)

	for i := range storeResp.Chunks {
		// Important: create local copies of loop variables for the goroutine
		chunkInfo := storeResp.Chunks[i]
		start := currentPos
		end := currentPos + chunkInfo.Size
		if end > fileSize {
			end = fileSize
		}
		chunkDataBytes := make([]byte, end-start)
		copy(chunkDataBytes, data[start:end])
		currentPos = end

		if len(chunkInfo.Replicas) == 0 {
			log.Printf("[Manager DFS Client] Warning: No replicas for chunk %d of key '%s'. Skipping.", chunkInfo.Id, key)
			continue
		}
		primaryReplicaAddr := chunkInfo.Replicas[0]

		wg.Add(1)
		go func(ci *common.ChunkInfo, primaryAddr string, chunkD []byte) {
			defer wg.Done()
			log.Printf("[Manager DFS Client] Uploading chunk %d (Size: %d) for key '%s' to primary %s...",
				ci.Id, len(chunkD), key, primaryAddr)

			var nodeErr error
			connNode, errDial := net.Dial("tcp", primaryAddr)
			if errDial != nil {
				nodeErr = fmt.Errorf("failed to dial primary replica %s for chunk %d: %w", primaryAddr, ci.Id, errDial)
			} else {
				nodeCli := client.NewClient(connNode)
				putChunkReq := &pnode.PutChunk{
					Chunk: &pnode.Chunk{
						Id:   ci.Id,
						Data: chunkD,
					},
					Replicas: ci.Replicas,
				}
				_, errSend := nodeCli.SendRequest(putChunkReq)
				nodeCli.Close()
				if errSend != nil {
					nodeErr = fmt.Errorf("failed to send PutChunk %d to replica %s: %w", ci.Id, primaryAddr, errSend)
				}
			}
			if nodeErr != nil {
				log.Printf("[Manager DFS Client] Error uploading chunk %d: %v", ci.Id, nodeErr)
				errChan <- nodeErr
			} else {
				log.Printf("[Manager DFS Client] Successfully sent chunk %d to %s", ci.Id, primaryAddr)
			}
		}(chunkInfo, primaryReplicaAddr, chunkDataBytes)
	}

	wg.Wait()
	close(errChan)

	// Check for errors
	for uploadErr := range errChan {
		if uploadErr != nil {
			return fmt.Errorf("[Manager DFS Client] failed to upload one or more chunks for key '%s': %w", key, uploadErr)
		}
	}

	// 4. Finish StoreRequest
	log.Printf("[Manager DFS Client] Sending finish StoreRequest for key '%s'", key)
	_, err = ctrlCli.SendRequest(&pclient.StoreRequest{
		Key:      key,
		Size:     fileSize,
		Finished: true,
	})
	if err != nil {
		return fmt.Errorf("[Manager DFS Client] finish StoreRequest for key '%s' failed: %w", key, err)
	}

	log.Printf("[Manager DFS Client] Successfully uploaded Key='%s'", key)
	return nil
}

// internalDownloadData handles downloading data bytes from the DFS from within the manager.
func (m *Manager) internalDownloadData(controllerClientAddr string, key string) ([]byte, error) {
	log.Printf("[Manager DFS Client] Downloading Key='%s' from %s", key, controllerClientAddr)

	// 1. Connect Controller Client Port
	connCtrl, err := net.Dial("tcp", controllerClientAddr)
	if err != nil {
		return nil, fmt.Errorf("[Manager DFS Client] failed to dial controller client port %s: %w", controllerClientAddr, err)
	}
	ctrlCli := client.NewClient(connCtrl)
	defer ctrlCli.Close()

	// 2. GetRequest
	getRespRaw, err := ctrlCli.SendRequest(&pclient.GetRequest{Key: key})
	if err != nil {
		// TODO: Differentiate KeyNotFound error from other errors?
		return nil, fmt.Errorf("[Manager DFS Client] GetRequest for key '%s' failed: %w", key, err)
	}
	getResp, ok := getRespRaw.(*pclient.GetResponse)
	if !ok {
		return nil, fmt.Errorf("[Manager DFS Client] unexpected response type for GetRequest: %T", getRespRaw)
	}
	if len(getResp.Chunks) == 0 {
		log.Printf("[Manager DFS Client] Key '%s' not found or is empty.", key)
		return []byte{}, nil // Return empty slice if no chunks
	}
	log.Printf("[Manager DFS Client] Received %d chunk locations for key '%s'", len(getResp.Chunks), key)

	// 3. Download Chunks from Storage Nodes (sequentially for simplicity)
	chunkDataMap := make(map[int][]byte, len(getResp.Chunks))
	var totalSize uint64
	var lastErr error // To store the last error when trying replicas

	for i := range getResp.Chunks {
		chunkInfo := getResp.Chunks[i] // Local copy
		totalSize += chunkInfo.Size
		if len(chunkInfo.Replicas) == 0 {
			return nil, fmt.Errorf("[Manager DFS Client] no replicas found for chunk %d (ID: %d) of key '%s'", i, chunkInfo.Id, key)
		}

		var chunkBytes []byte
		success := false
		log.Printf("[Manager DFS Client] Downloading chunk %d (ID: %d, Size: %d)...", i, chunkInfo.Id, chunkInfo.Size)

		for _, replicaAddr := range chunkInfo.Replicas {
			log.Printf("[Manager DFS Client] Attempting download chunk %d from replica %s", chunkInfo.Id, replicaAddr)
			connNode, errDial := net.Dial("tcp", replicaAddr)
			if errDial != nil {
				lastErr = fmt.Errorf("failed dial replica %s for chunk %d: %w", replicaAddr, chunkInfo.Id, errDial)
				log.Printf("[Manager DFS Client] %v", lastErr)
				continue
			}
			nodeCli := client.NewClient(connNode)

			getChunkRespRaw, errSend := nodeCli.SendRequest(&pnode.GetChunk{Id: chunkInfo.Id})
			nodeCli.Close()
			if errSend != nil {
				lastErr = fmt.Errorf("failed GetChunk %d from replica %s: %w", chunkInfo.Id, replicaAddr, errSend)
				log.Printf("[Manager DFS Client] %v", lastErr)
				continue
			}
			chunk, ok := getChunkRespRaw.(*pnode.Chunk)
			if !ok {
				lastErr = fmt.Errorf("unexpected response type %T for GetChunk %d from %s", getChunkRespRaw, chunkInfo.Id, replicaAddr)
				log.Printf("[Manager DFS Client] %v", lastErr)
				continue
			}
			if chunk.Size != nil && uint64(len(chunk.Data)) != *chunk.Size {
				lastErr = fmt.Errorf("chunk %d size mismatch from %s (got %d, expected %d)", chunkInfo.Id, replicaAddr, len(chunk.Data), *chunk.Size)
				log.Printf("[Manager DFS Client] %v", lastErr)
				continue
			}
			// TODO: Add optional hash verification here if chunk.Hash is set

			chunkBytes = chunk.Data
			success = true
			log.Printf("[Manager DFS Client] Successfully downloaded chunk %d from %s", chunkInfo.Id, replicaAddr)
			break // Downloaded successfully from this replica
		}

		if !success {
			return nil, fmt.Errorf("[Manager DFS Client] failed to download chunk %d (ID: %d) for key '%s' from all replicas. Last error: %w", i, chunkInfo.Id, key, lastErr)
		}
		chunkDataMap[i] = chunkBytes
	}

	// 4. Assemble Data
	var fileDataBuffer bytes.Buffer
	fileDataBuffer.Grow(int(totalSize))
	for i := 0; i < len(getResp.Chunks); i++ {
		if data, ok := chunkDataMap[i]; ok {
			fileDataBuffer.Write(data)
		} else {
			return nil, fmt.Errorf("[Manager DFS Client] internal error: missing data for chunk index %d of key '%s'", i, key)
		}
	}

	finalData := fileDataBuffer.Bytes()
	if uint64(len(finalData)) != totalSize {
		log.Printf("[Manager DFS Client] Warning: Final data size %d != expected %d for key '%s'", len(finalData), totalSize, key)
	}

	log.Printf("[Manager DFS Client] Successfully downloaded Key='%s', Size=%d", key, len(finalData))
	return finalData, nil
}

// scheduleMapTask finds a suitable node and sends the MapTaskRequest.
// Returns true if successfully scheduled, false otherwise.
func (m *Manager) scheduleMapTask(job *MapReduceJobState, taskID uint32) bool {
	job.mu.Lock()
	mapTask := job.MapTasks[taskID]
	if mapTask.Status != TaskStatusPending {
		job.mu.Unlock()
		log.Printf("[Job %s] Map task %d already scheduled/completed.", job.JobID, taskID)
		return true // Consider it 'handled' for scheduling purposes
	}
	job.mu.Unlock()

	// Get the chunk ID from the map task
	inputChunkID := mapTask.InputChunkID

	// Find nodes that have this chunk for data locality, or select based on load
	m.mu.Lock()

	// Try to find nodes that have the chunk data (data locality)
	nodesWithChunk := make([]*StorageNode, 0)
	availableNodes := make([]*StorageNode, 0, len(m.nodes))

	for _, node := range m.nodes {
		// Basic check: is node alive
		if time.Since(node.LastHeartbeat) < 30*time.Second {
			availableNodes = append(availableNodes, node)

			// Check if node has the chunk
			if _, hasChunk := node.chunks[inputChunkID]; hasChunk {
				nodesWithChunk = append(nodesWithChunk, node)
			}
		}
	}

	m.mu.Unlock()

	if len(availableNodes) == 0 {
		log.Printf("[Job %s] No available nodes to schedule map task %d.", job.JobID, taskID)
		return false
	}

	var nodeToUse *StorageNode

	// Select node based on data locality if possible, otherwise by weight (load balance)
	if len(nodesWithChunk) > 0 {
		// Sort nodes that have the chunk by weight (higher weight = more available resources)
		sort.Slice(nodesWithChunk, func(i, j int) bool {
			return nodesWithChunk[i].weight() > nodesWithChunk[j].weight()
		})
		nodeToUse = nodesWithChunk[0]
		log.Printf("[Job %s] Map task %d using data-local node %s (weight: %.2f)",
			job.JobID, taskID, nodeToUse.ID, nodeToUse.weight())
	} else {
		// Sort all available nodes by weight
		sort.Slice(availableNodes, func(i, j int) bool {
			return availableNodes[i].weight() > availableNodes[j].weight()
		})
		nodeToUse = availableNodes[0]
		log.Printf("[Job %s] Map task %d using highest-weight node %s (weight: %.2f)",
			job.JobID, taskID, nodeToUse.ID, nodeToUse.weight())
	}

	// Configure map task request
	outputKeyPrefix := fmt.Sprintf("%s/%s-map-%d", job.TempDir, job.JobID, taskID)

	// Prepare request
	req := &node.MapTaskRequest{
		JobId:           job.JobID,
		TaskId:          taskID,
		MapFuncId:       job.MapFuncID,
		InputChunkId:    mapTask.InputChunkID,
		NumReduceTasks:  job.NumReduceTasks,
		OutputKeyPrefix: outputKeyPrefix,
	}

	// Send request asynchronously
	go func(targetNode *StorageNode, taskReq *node.MapTaskRequest) {
		log.Printf("[Job %s] Sending map task %d to node %s", job.JobID, taskID, targetNode.ID)
		_, err := targetNode.cli.SendRequest(taskReq)
		if err != nil {
			log.Printf("[Job %s] Failed to send map task %d to node %s: %v", job.JobID, taskID, targetNode.ID, err)
			// Handle failure: Mark task as pending again? Retry?
			job.mu.Lock()
			if job.MapTasks[taskID].Status == TaskStatusRunning { // Only revert if it was marked running
				job.MapTasks[taskID].Status = TaskStatusPending
				job.MapTasks[taskID].AssignedNodeID = ""
				// Re-queue the task? May have infinite loops.
				// job.PendingMapTasks <- taskID
			}
			job.mu.Unlock()
		} else {
			log.Printf("[Job %s] Successfully sent map task %d to node %s", job.JobID, taskID, targetNode.ID)
			// Optionally mark as running here, or wait for node confirmation?
			// Let's mark as running optimistically for now.
			job.mu.Lock()
			mapTask := job.MapTasks[taskID]
			if mapTask.Status == TaskStatusPending { // Check status before updating
				mapTask.Status = TaskStatusRunning
				mapTask.AssignedNodeID = targetNode.ID
				mapTask.StartTime = time.Now()
			}
			job.mu.Unlock()
		}
	}(nodeToUse, req)

	// Mark as 'scheduled' (request sent), actual running state updated in goroutine/completion handler.
	// Let's update status optimistically here for now
	job.mu.Lock()
	if job.MapTasks[taskID].Status == TaskStatusPending {
		job.MapTasks[taskID].Status = TaskStatusRunning // Mark as running immediately after scheduling attempt
		job.MapTasks[taskID].AssignedNodeID = nodeToUse.ID
		job.MapTasks[taskID].StartTime = time.Now()
	}
	job.mu.Unlock()

	return true
}

// scheduleReduceTask finds a suitable node and sends the ReduceTaskRequest.
// Returns true if successfully scheduled, false otherwise.
func (m *Manager) scheduleReduceTask(job *MapReduceJobState, taskID uint32) bool {
	job.mu.Lock()
	reduceTask := job.ReduceTasks[taskID]
	if reduceTask.Status != TaskStatusPending {
		job.mu.Unlock()
		log.Printf("[Job %s] Reduce task %d already scheduled/completed/failed.", job.JobID, taskID)
		return true // Consider it 'handled' for scheduling purposes
	}
	// Get intermediate keys assigned to this reducer
	intermediateKeys := job.mapOutputsPerReducer[taskID]
	job.mu.Unlock()

	if len(intermediateKeys) == 0 {
		// If a reducer has no input keys, it means no map tasks produced output for it.
		// We can mark it as completed immediately.
		log.Printf("[Job %s] Reduce task %d has no intermediate keys, marking as completed.", job.JobID, taskID)
		job.mu.Lock()
		if reduceTask.Status == TaskStatusPending { // Double check status before marking completed
			reduceTask.Status = TaskStatusCompleted
			reduceTask.StartTime = time.Now() // Or skip times?
			reduceTask.EndTime = time.Now()
			job.completedReduceTasks.Add(1)
			// Check if this completion finishes the job
			if job.completedReduceTasks.Load() == job.NumReduceTasks && job.Status == JobStatusRunning {
				log.Printf("[Job %s] All reduce tasks completed (including empty ones). Job finished successfully.", job.JobID)
				job.Status = JobStatusCompleted
				job.EndTime = time.Now()
			}
		}
		job.mu.Unlock()
		return true
	}

	// Select a node based on load (use weight for load balancing)
	m.mu.Lock()
	availableNodes := make([]*StorageNode, 0, len(m.nodes))
	for _, node := range m.nodes {
		if time.Since(node.LastHeartbeat) < 30*time.Second {
			availableNodes = append(availableNodes, node)
		}
	}
	m.mu.Unlock()

	if len(availableNodes) == 0 {
		log.Printf("[Job %s] No available nodes to schedule reduce task %d.", job.JobID, taskID)
		return false
	}

	// Sort nodes by weight (higher weight means more available resources)
	sort.Slice(availableNodes, func(i, j int) bool {
		return availableNodes[i].weight() > availableNodes[j].weight()
	})

	// Choose the node with the highest weight (most available resources)
	nodeToUse := availableNodes[0]
	log.Printf("[Job %s] Reduce task %d using highest-weight node %s (weight: %.2f)",
		job.JobID, taskID, nodeToUse.ID, nodeToUse.weight())

	// Set output key for this reduce task
	var outputKey string
	if job.ControllerAggregate {
		// If the controller will aggregate results later, put reduce output in temp
		outputKey = fmt.Sprintf("%s/%s-reduce-%d", job.TempDir, job.JobID, taskID)
	} else {
		// Otherwise use the final output key format but still put in temp
		outputKey = fmt.Sprintf("%s/%s-output-reduce-%d", job.TempDir, job.JobID, taskID)
	}

	// Prepare request
	req := &node.ReduceTaskRequest{
		JobId:            job.JobID,
		TaskId:           taskID,
		ReduceFuncId:     job.ReduceFuncID,
		IntermediateKeys: intermediateKeys,
		FinalOutputKey:   outputKey, // Use the new outputKey
	}

	// Send request asynchronously and update task status optimistically
	job.mu.Lock()
	if job.ReduceTasks[taskID].Status == TaskStatusPending {
		job.ReduceTasks[taskID].Status = TaskStatusRunning
		job.ReduceTasks[taskID].AssignedNodeID = nodeToUse.ID
		job.ReduceTasks[taskID].StartTime = time.Now()
		log.Printf("[Job %s] Marked reduce task %d as Running, assigned to %s", job.JobID, taskID, nodeToUse.ID)
	} else {
		// Task status changed while we were preparing, don't send
		log.Printf("[Job %s] Reduce task %d status changed before sending request (Status: %v). Aborting send.", job.JobID, taskID, job.ReduceTasks[taskID].Status)
		job.mu.Unlock()
		return true // Consider handled
	}
	job.mu.Unlock()

	go func(targetNode *StorageNode, taskReq *node.ReduceTaskRequest) {
		log.Printf("[Job %s] Sending reduce task %d to node %s", job.JobID, taskID, targetNode.ID)
		//Send request to selected node
		_, err := targetNode.cli.SendRequest(taskReq)
		if err != nil {
			log.Printf("[Job %s] Failed to send reduce task %d to node %s: %v. Marking as pending for retry.", job.JobID, taskID, targetNode.ID, err)
			job.mu.Lock()
			if job.ReduceTasks[taskID].Status == TaskStatusRunning && job.ReduceTasks[taskID].AssignedNodeID == targetNode.ID {
				job.ReduceTasks[taskID].Status = TaskStatusPending // Revert status
				job.ReduceTasks[taskID].AssignedNodeID = ""
				job.ReduceTasks[taskID].StartTime = time.Time{} // Clear start time
				// Re-queue? Need careful handling of channel for retries
				// Consider adding to a separate retry mechanism or letting the main loop retry.
			}
			job.mu.Unlock()
		} else {
			log.Printf("[Job %s] Successfully sent reduce task %d to node %s", job.JobID, taskID, targetNode.ID)
			// Status already marked as Running
		}
	}(nodeToUse, req)

	return true
}

// HandleTaskCompletion processes the TaskCompletedRequest from a storage node.
func (m *Manager) HandleTaskCompletion(req *node.TaskCompletedRequest) error {
	m.mu.Lock()
	job, ok := m.mapReduceJobs[req.JobId]
	m.mu.Unlock() // Release manager lock, use job lock

	if !ok {
		log.Printf("[Controller] Received completion for unknown job ID: %s", req.JobId)
		return fmt.Errorf("job %s not found", req.JobId)
	}

	job.mu.Lock()
	defer job.mu.Unlock()

	log.Printf("[Job %s] Received TaskCompleted: TaskID=%d, IsMap=%t, Success=%t",
		req.JobId, req.TaskId, req.IsMapTask, req.Success)

	if req.IsMapTask {
		mapTask, ok := job.MapTasks[req.TaskId]
		if !ok {
			log.Printf("[Job %s] Received completion for unknown map task ID: %d", job.JobID, req.TaskId)
			return fmt.Errorf("map task %d not found in job %s", req.TaskId, job.JobID)
		}

		if mapTask.Status != TaskStatusRunning {
			log.Printf("[Job %s] Received completion for map task %d which is not in running state (Status: %v)", job.JobID, req.TaskId, mapTask.Status)
			// Ignore stale/duplicate completion reports?
			return nil
		}

		mapTask.EndTime = time.Now()
		if req.Success {
			mapTask.Status = TaskStatusCompleted
			mapTask.OutputKeys = req.OutputKeys
			job.completedMapTasks.Add(1)

			// Distribute intermediate keys to reducer bins
			// Simple partitioning based on hash of intermediate key modulo numReducers
			// In a real system, the mapper would likely partition this.
			// assume output keys are named like 'prefix-reducerIdx-somehash'
			// just assign based on hash.
			for _, intermediateKey := range mapTask.OutputKeys {
				// Example partitioning: hash the key
				h := fnv.New32a()
				h.Write([]byte(intermediateKey))
				reducerIdx := h.Sum32() % job.NumReduceTasks
				job.mapOutputsPerReducer[reducerIdx] = append(job.mapOutputsPerReducer[reducerIdx], intermediateKey)
			}

			log.Printf("[Job %s] Map task %d completed successfully. Total completed: %d/%d",
				job.JobID, req.TaskId, job.completedMapTasks.Load(), job.NumMapTasks)

			// Check if all map tasks are done
			if job.completedMapTasks.Load() == job.NumMapTasks {
				log.Printf("[Job %s] All map tasks completed. Scheduling reduce tasks.", job.JobID)
				// Schedule reduce tasks
				for reduceTaskID := range job.ReduceTasks {
					job.PendingReduceTasks <- reduceTaskID
				}
				close(job.PendingReduceTasks)
			}
		} else {
			mapTask.Status = TaskStatusFailed
			mapTask.Error = req.ErrorMessage
			log.Printf("[Job %s] Map task %d failed: %s", job.JobID, req.TaskId, req.ErrorMessage)
			// TODO: Handle map task failure (e.g., retry, fail job)
			job.Status = JobStatusFailed // Fail entire job for now
			job.Error = fmt.Sprintf("Map task %d failed: %s", req.TaskId, req.ErrorMessage)
		}
	} else { // Reduce Task Completion
		reduceTask, ok := job.ReduceTasks[req.TaskId]
		if !ok {
			log.Printf("[Job %s] Received completion for unknown reduce task ID: %d", job.JobID, req.TaskId)
			return fmt.Errorf("reduce task %d not found in job %s", req.TaskId, job.JobID)
		}

		if reduceTask.Status != TaskStatusRunning {
			log.Printf("[Job %s] Received completion for reduce task %d which is not in running state (Status: %v)", job.JobID, req.TaskId, reduceTask.Status)
			return nil // Ignore stale/duplicate completion reports?
		}

		reduceTask.EndTime = time.Now()
		if req.Success {
			reduceTask.Status = TaskStatusCompleted
			// req.OutputKeys should contain the single final output key for this reducer
			if len(req.OutputKeys) == 1 {
				reduceTask.OutputKey = req.OutputKeys[0]
			} else {
				log.Printf("[Job %s] Reduce task %d completed but reported %d output keys (expected 1). Using predefined key: %s",
					job.JobID, req.TaskId, len(req.OutputKeys), reduceTask.OutputKey)
			}
			job.completedReduceTasks.Add(1)

			log.Printf("[Job %s] Reduce task %d completed successfully. Total completed: %d/%d",
				job.JobID, req.TaskId, job.completedReduceTasks.Load(), job.NumReduceTasks)

			// Check if all reduce tasks are done or if job has failed
			if job.completedReduceTasks.Load() == job.NumReduceTasks {
				log.Printf("[Job %s] All reduce tasks completed. Job finished successfully.", job.JobID)
				job.Status = JobStatusCompleted
				job.EndTime = time.Now()

				// 如果不需要聚合，需要将temp目录中的输出文件复制到最终位置
				if !job.ControllerAggregate {
					// 复制每个reducer的输出到最终位置，然后清理
					for reduceTaskID, reduceTaskState := range job.ReduceTasks {
						tempOutputKey := reduceTaskState.OutputKey
						finalOutputKey := fmt.Sprintf("%s-reduce-%d", job.OutputKey, reduceTaskID)

						// 复制文件
						log.Printf("[Job %s] Copying reducer output from '%s' to final destination '%s'",
							job.JobID, tempOutputKey, finalOutputKey)

						// 获取控制器客户端地址
						controllerClientAddr := getControllerClientAddr()

						// 下载中间文件
						data, err := m.internalDownloadData(controllerClientAddr, tempOutputKey)
						if err != nil {
							log.Printf("[Job %s] Warning: Failed to download reducer output '%s': %v",
								job.JobID, tempOutputKey, err)
							continue
						}

						// 上传到最终位置
						err = m.internalUploadData(controllerClientAddr, finalOutputKey, data)
						if err != nil {
							log.Printf("[Job %s] Warning: Failed to upload final output '%s': %v",
								job.JobID, finalOutputKey, err)
						}
					}

					// 清理所有中间文件
					m.cleanupJobFiles(job)
				}
			}
		} else {
			reduceTask.Status = TaskStatusFailed
			reduceTask.Error = req.ErrorMessage
			log.Printf("[Job %s] Reduce task %d failed: %s", job.JobID, req.TaskId, req.ErrorMessage)
			// TODO: Handle reduce task failure
			job.Status = JobStatusFailed
			job.Error = fmt.Sprintf("Reduce task %d failed: %s", req.TaskId, req.ErrorMessage)
		}

		// 如果作业失败，也清理中间文件
		if job.Status == JobStatusFailed {
			m.cleanupJobFiles(job)
		}
	}

	return nil
}

// getControllerClientAddr
func getControllerClientAddr() string {
	// use port 8080
	return "localhost:8080"
}

// Add a cleanup function after the task is successful
func (m *Manager) cleanupJobFiles(job *MapReduceJobState) {
	log.Printf("[Job %s] Cleaning up all intermediate files...", job.JobID)

	// 列出所有该任务的中间文件
	tempFiles, err := m.ListFiles(fmt.Sprintf("%s/%s", job.TempDir, job.JobID))
	if err != nil {
		log.Printf("[Job %s] Warning: Failed to list intermediate files: %v", job.JobID, err)
		return
	}

	// 删除所有中间文件
	for _, file := range tempFiles {
		log.Printf("[Job %s] Deleting intermediate file '%s'...", job.JobID, file.Name)
		_, errDel := m.DeleteFile(file.Name)
		if errDel != nil {
			log.Printf("[Job %s] Warning: Failed to delete intermediate file '%s': %v", job.JobID, file.Name, errDel)
		}
	}

	log.Printf("[Job %s] Cleanup complete.", job.JobID)
}

// --- Add new methods to Manager ---
// StorePlugin saves the plugin code to local disk and updates registry
func (m *Manager) StorePlugin(pluginId string, pluginCode []byte) error {
	if pluginId == "" {
		return errors.New("plugin ID cannot be empty")
	}
	// TODO: Validate pluginId format? Sanitize?
	// TODO: Validate pluginCode? (Basic checks?)

	m.mu.Lock()
	defer m.mu.Unlock()

	// Define filepath (ensure it's unique and safe)
	// Using pluginId directly might have issues with special chars, sanitize it.
	safeFilename := pluginId + ".so" // Simple suffix, assumes Linux/macOS for now
	pluginPath := filepath.Join(m.pluginDir, safeFilename)

	// Write file
	err := os.WriteFile(pluginPath, pluginCode, 0644)
	if err != nil {
		return fmt.Errorf("failed to write plugin file %s: %w", pluginPath, err)
	}

	// Update registry (overwrite if exists)
	m.pluginRegistry[pluginId] = pluginPath
	log.Printf("Stored plugin '%s' at %s", pluginId, pluginPath)
	return nil
}

// GetPlugin retrieves the plugin code from local disk
func (m *Manager) GetPlugin(pluginId string) (code []byte, found bool, err error) {
	m.mu.Lock()
	pluginPath, exists := m.pluginRegistry[pluginId]
	m.mu.Unlock() // Unlock before potential disk I/O

	if !exists {
		return nil, false, nil // Not found, no error
	}

	code, err = os.ReadFile(pluginPath)
	if err != nil {
		if os.IsNotExist(err) {
			// Registry has it, but file is missing? Inconsistency.
			log.Printf("Error: Plugin '%s' found in registry but file %s is missing: %v", pluginId, pluginPath, err)
			// Optionally remove from registry here?
			return nil, false, fmt.Errorf("plugin file missing on controller for id %s", pluginId)
		}
		return nil, false, fmt.Errorf("failed to read plugin file %s: %w", pluginPath, err)
	}

	return code, true, nil // Found and read successfully
}
